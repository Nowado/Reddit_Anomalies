{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#nltk.download() #use if nltk cries for any language-specific docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defaults for my own usecase, replace all accordingly\n",
    "red_model_location = 'Models\\word2vec\\English_CoNLL17\\ '#location of reference model\n",
    "red_model_name = 'model.bin' #location of reference model binary\n",
    "subreddits_json_location = 'Subs' #location of subreddit jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ay=str()\n",
    "for file in os.listdir('Models\\word2vec\\English CoNLL17 corpus - gensim skipgram - 10'):\n",
    "    full_filename = \"%s/%s\" % ('Models\\word2vec\\English CoNLL17 corpus - gensim skipgram - 10', file)\n",
    "    if full_filename.endswith('.txt'):\n",
    "        with open(full_filename,'r', encoding=\"ISO-8859-1\") as fi:\n",
    "            ay=fi.readlines()\n",
    "\"\"\"\n",
    "subs=[]\n",
    "for filename in os.listdir(subreddits_json_location):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(subreddits_json_location, filename)) as json_file:\n",
    "            json_text = json.load(json_file)\n",
    "            print(filename)\n",
    "            subs.append(json_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not used\n",
    "def sub2vocab(sub):\n",
    "    documents=sub2docs(sub)\n",
    "    temp=[]\n",
    "    stop = set(stopwords.words('english'))\n",
    "    #for doc in documents:\n",
    "        #print(word_tokenize(doc))\n",
    "    texts =[\n",
    "        [word for word in document.lower().split() if word not in stop]\n",
    "        for document in documents\n",
    "    ]\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    texts = [\n",
    "        [token for token in text if frequency[token] > 1]\n",
    "        for text in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not used even more\n",
    "\"\"\"def sub2vocab(sub):\n",
    "    raw=str()\n",
    "    for i, j in sub.items():\n",
    "        raw+=i\n",
    "        raw+=' '\n",
    "        for o,p in j.items():\n",
    "            raw+=p\n",
    "            raw+=' '\n",
    "        raw+='\\n'\n",
    "    temp=[]\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for i in word_tokenize(raw.lower()):\n",
    "        if i not in stop:\n",
    "            temp.append(i)\n",
    "    frequency = defaultdict(int)\n",
    "    for word in temp:\n",
    "        frequency[word] += 1\n",
    "    temp = set([token for token in temp if frequency[token] > 1])\n",
    "    return temp\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sub2docs(sub):\n",
    "    documents=[]\n",
    "    trans_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    for i, j in sub.items():\n",
    "        temp=re.sub(r'\\d+','',i.lower().translate(trans_table)).replace('\\n', '')\n",
    "        documents.append(temp)\n",
    "        for o,p in j.items():\n",
    "            temp=re.sub(r'\\d+', '',p.lower().translate(trans_table)).replace('\\n', '')\n",
    "            documents.append(temp)\n",
    "    frequency = defaultdict(int)\n",
    "    for document in documents:\n",
    "        for token in document.split():\n",
    "            frequency[token] += 1\n",
    "    documents = [\n",
    "        [token for token in document.split() if frequency[token] > 1]\n",
    "        for document in documents]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(words):\n",
    "    trans_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return set([s.translate(trans_table) for s in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_wv2distances(wv):\n",
    "    dists=np.zeros((len(wv.vocab),len(wv.vocab)))\n",
    "    words=[]\n",
    "    for word in wv.vocab.keys():\n",
    "        words.append(word)\n",
    "    start=0\n",
    "    for i in range(len(keys)):\n",
    "        print(i,len(keys))\n",
    "        for j in range(len(keys)):\n",
    "            j+=start\n",
    "            if j<len(keys):\n",
    "                dists[i,j]=wv.n_similarity([words[i]],[words[j]])\n",
    "        start+=1\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_wv2distances(target_wv,reference_wv):\n",
    "    dists=np.zeros((len(target_wv.vocab),len(target_wv.vocab)))\n",
    "    words=[]\n",
    "    for word in target_wv.vocab.keys():\n",
    "        words.append(word)\n",
    "    start=0\n",
    "    for i in range(len(keys)):\n",
    "        print(i,len(keys))\n",
    "        for j in range(len(keys)):\n",
    "            j+=start\n",
    "            try:\n",
    "                if j<len(keys):\n",
    "                    dists[i,j]=word_vectors.n_similarity([words[i]],[words[j]])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "                #print(\"type error: \" + str(e))\n",
    "        start+=1\n",
    "    return dists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2wv(docs,name='model'):\n",
    "    model = Word2Vec(docs, size=300, window=5, workers=3, sg=1, min_count=1, negative=8)\n",
    "    if name!=None:\n",
    "        model.save(name+\".model\")\n",
    "    word_vectors = model.wv\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diagonal(dists):\n",
    "    for i in range(dists.shape[0]):\n",
    "        for j in range(dists.shape[1]):\n",
    "            if i==j:\n",
    "                dists[i,j]=0\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_wv=docs2wv(sub2docs(subs[0]),name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_wv=gensim.models.KeyedVectors.load_word2vec_format(os.path.join(os.path.dirname(red_model_name), red_model_location), binary=True).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_ref=ref_wv2distances(target_wv,ref_wv)\n",
    "dists_t=target_wv2distances(target_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_ref = remove_diagonal(dists_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_t  = remove_diagonal(dists_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks=pd.DataFrame(columns=['Value'])\n",
    "i=0\n",
    "dists_comp=dists_t-dists_ref\n",
    "for word in target_wv.vocab.keys():\n",
    "    temp=np.sum(dists_comp[i,:])+np.sum(dists_comp[:,i])\n",
    "    ranks.loc[word]=temp\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks.sort_values(['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
